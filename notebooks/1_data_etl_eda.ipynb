{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 游닀 Instructions\n",
    "The idea of this challenge is to identify in a better way your capacities to translate data into assets, we expect a good pipeline and solution that you can understand and translate. Select one of the below problems where it is affordable to create a REST API using python (scripts files) where we can identify the data pipeline/dataflow, but you can develop a MD file. Remember to Make it available over github \n",
    "\n",
    "#### Hint:\n",
    "- Highlight variables or patterns using EDA\n",
    "- Validate your functions\n",
    "- Be clear with the pipeline\n",
    "- Readme and Requirements files are expected\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import xlrd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "#print(os.getcwd())\n",
    "ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(\"ds_proj1\")))\n",
    "os.chdir(ROOT_DIR)\n",
    "print(os.getcwd())\n",
    "\n",
    "#from scripts import ptools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect links\n",
    "web_url = 'https://www.cityobservatory.birmingham.gov.uk/@birmingham-city-council/purchase-card-transactions/'\n",
    "response = requests.get(web_url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# Find all 'a' tags (links) in the HTML\n",
    "links = soup.find_all('a')\n",
    "\n",
    "# Extract links with the specified extension and pattern\n",
    "all_links = [urljoin(web_url, link.get('href')) for link in links if link.get('href') and link.get('href').endswith('.xls')]\n",
    "xls_links = list(filter(lambda link: '.datopian.' in link, all_links))\n",
    "\n",
    "# Print the links ending with the specified extension\n",
    "if xls_links:\n",
    "    print(f\"Links ending with {'.xls'} extension:\")\n",
    "    for idx, link in enumerate(xls_links, start=1):\n",
    "        print(f\"{idx}. {link}\")\n",
    "\n",
    "#print(xls_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# List of URLs\n",
    "urls = xls_links.copy()\n",
    "\n",
    "# Funci칩n completa que integra la extracci칩n de fechas de una lista de URLs y encuentra los 칰ltimos 6 meses disponibles\n",
    "def extract_dates_and_find_last_6_months(urls):\n",
    "    # Diccionario para mapear nombres de meses posiblemente incompletos a nombres completos\n",
    "    months = {\n",
    "        'jan': 'January', 'feb': 'February', 'mar': 'March', 'apr': 'April', \n",
    "        'may': 'May', 'jun': 'June', 'jul': 'July', 'aug': 'August', \n",
    "        'sep': 'September', 'oct': 'October', 'nov': 'November', 'dec': 'December'\n",
    "    }\n",
    "\n",
    "    # Funci칩n para extraer y formatear las fechas\n",
    "    def format_year_month(url):\n",
    "        file_name = url.split('/')[-1].lower()\n",
    "        match = re.search(r'(\\w+)(\\d{2,4})', file_name.replace('-', ''))\n",
    "        if match:\n",
    "            month_part, year = match.group(1), match.group(2)\n",
    "            year = f\"20{year}\" if len(year) == 2 else year\n",
    "\n",
    "            for short_month, full_month in months.items():\n",
    "                if short_month in month_part:\n",
    "                    return f\"{year} {full_month}\"\n",
    "\n",
    "            # Intento alternativo para extraer el mes\n",
    "            month_match = re.search(r'(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)', month_part)\n",
    "            return f\"{year} {months[month_match.group(0)]}\" if month_match else None\n",
    "        return None\n",
    "\n",
    "    # Extraer y formatear las fechas de las URLs\n",
    "    formatted_dates = [format_year_month(url) for url in urls]\n",
    "\n",
    "    # Obtener la fecha actual\n",
    "    current_date = datetime.now()\n",
    "    current_year = current_date.year\n",
    "    current_month = current_date.strftime(\"%B\")\n",
    "\n",
    "    # Funci칩n para obtener los 칤ndices de los 칰ltimos 6 meses disponibles\n",
    "    def get_last_6_months_indices(dates, current_year, current_month):\n",
    "        valid_dates = [(i, date) for i, date in enumerate(dates) if date is not None]\n",
    "        date_objects = [(index, datetime.strptime(date, '%Y %B')) for index, date in valid_dates]\n",
    "        date_objects.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        last_6_months_indices = []\n",
    "        for index, date in date_objects:\n",
    "            if date.year == current_year and date.strftime(\"%B\") == current_month:\n",
    "                last_6_months_indices.append(index)\n",
    "                if len(last_6_months_indices) == 6:\n",
    "                    break\n",
    "            elif date < datetime(current_year, current_date.month, 1):\n",
    "                last_6_months_indices.append(index)\n",
    "                if len(last_6_months_indices) == 6:\n",
    "                    break\n",
    "\n",
    "        return last_6_months_indices\n",
    "\n",
    "    # Encontrar y devolver los 칤ndices de los 칰ltimos 6 meses disponibles\n",
    "    return get_last_6_months_indices(formatted_dates, current_year, current_month)\n",
    "\n",
    "\n",
    "# Identificar los 칤ndices de los 칰ltimos 6 meses disponibles\n",
    "last_6_months_indices = extract_dates_and_find_last_6_months(urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_6_months_urls = [urls[i] for i in last_6_months_indices]\n",
    "last_6_months_urls\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select time window for analysis\n",
    "Last Six months available "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-period data \n",
    "import os\n",
    "to_download = last_6_months_urls.copy()\n",
    "\n",
    "\n",
    "def download_documents(links, destination_directory='downloads'):\n",
    "    \"\"\"\n",
    "    Downloads documents from a list of direct download links.\n",
    "\n",
    "    Parameters:\n",
    "        links (list): List of direct download links.\n",
    "        destination_directory (str): Directory where downloaded documents will be saved.\n",
    "\n",
    "    Returns:\n",
    "        list: List of paths to the downloaded documents.\n",
    "    \"\"\"\n",
    "    # Create the destination directory if it doesn't exist\n",
    "    if not os.path.exists(destination_directory):\n",
    "        os.makedirs(destination_directory)\n",
    "\n",
    "    document_paths = []\n",
    "\n",
    "    for idx, link in enumerate(links, start=1):\n",
    "        try:\n",
    "            # Get the content of the document\n",
    "            response = requests.get(link)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Get the filename from the link\n",
    "            file_name = os.path.join(destination_directory, f\"document_{idx}.xls\")\n",
    "\n",
    "            # Save the content to a local file\n",
    "            with open(file_name, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "\n",
    "            document_paths.append(file_name)\n",
    "            print(f\"Document {idx} downloaded successfully: {file_name}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error downloading document {idx} from {link}: {e}\")\n",
    "\n",
    "    return document_paths\n",
    "\n",
    "\n",
    "downloaded_documents = download_documents(to_download, destination_directory= 'data/raw')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "#print(os.getcwd())\n",
    "\n",
    "\n",
    "def load_and_concatenate_xls(directory=\"data/raw/\"):\n",
    "    \"\"\"\n",
    "    Load and concatenate Excel (xls) files into a single DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        directory (str): Directory containing the xls files.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing combined information from all xls files.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".xls\"):\n",
    "            # Build the full file path\n",
    "            file_path = os.path.join(directory, file)\n",
    "\n",
    "            # Read the Excel file into a DataFrame\n",
    "            df = pd.read_excel(file_path)\n",
    "\n",
    "            # Add the DataFrame to the list of DataFrames\n",
    "            frames.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into one\n",
    "    result = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Call the function to load and concatenate the xls files\n",
    "rawdata = load_and_concatenate_xls()\n",
    "\n",
    "rawdata.to_csv('data/interim/raw_aggregate.csv', index=False, encoding='utf-8')\n",
    "rawdata.sample(1000).to_csv('data/interim/to_test.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating raw data copy\n",
    "df = rawdata.copy()\n",
    "\n",
    "# Basic information about the dataset\n",
    "data_info = df.info()\n",
    "\n",
    "# Checking for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Descriptive statistics for numerical features\n",
    "numerical_stats = df.describe()\n",
    "# Checking clean shape and n췈 of employees\n",
    "rows, cols = df.shape \n",
    "director = df.Directorate.unique().shape[0]\n",
    "\n",
    "# Checking unique values for categorical features\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "unique_values = df[categorical_columns].nunique()\n",
    "\n",
    "# Summarizing the analysis\n",
    "print(data_info, missing_values, numerical_stats, unique_values.to_frame(name='Unique Values'))\n",
    "\n",
    "# Visualize missing values with datetime on the y-axis\n",
    "df['TRANS DATE'] = pd.to_datetime(df['TRANS DATE'])\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.heatmap(rawdata.set_index('TRANS DATE').isnull().transpose(), cmap='viridis',  cbar=False)\n",
    "# format the x-axis tick labels as dates\n",
    "plt.xticks(rotation=90)\n",
    "#plt.gca().set_xticklabels(data['TRANS DATE'].data.strftime('%Y-%m'))\n",
    "plt.title('Missing Values Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection for 'ORIGINAL GROSS AMT'\n",
    "Q1 = df['ORIGINAL GROSS AMT'].quantile(0.25)\n",
    "Q3 = df['ORIGINAL GROSS AMT'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Defining bounds for outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filtering out the outliers\n",
    "data_no_outliers = df[\n",
    "    (df['ORIGINAL GROSS AMT'] >= lower_bound) & \n",
    "    (df['ORIGINAL GROSS AMT'] <= upper_bound)\n",
    "]\n",
    "\n",
    "# Plotting the original and outlier-removed distributions for comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(data=df['ORIGINAL GROSS AMT'])\n",
    "plt.title('Original Data')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=data_no_outliers['ORIGINAL GROSS AMT'])\n",
    "plt.title('Data Without Outliers')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA: Creating visualizations for business-related insights\n",
    "\n",
    "# Day of the Week\n",
    "data_no_outliers['Day of Week'] = data_no_outliers['TRANS DATE'].dt.day_name()\n",
    "\n",
    "# Month\n",
    "data_no_outliers['Month'] = data_no_outliers['TRANS DATE'].dt.month_name()\n",
    "\n",
    "# Part of Month (Early: 1-10, Mid: 11-20, Late: 21-end)\n",
    "def part_of_month(day):\n",
    "    if day <= 10:\n",
    "        return 'Early'\n",
    "    elif day <= 20:\n",
    "        return 'Mid'\n",
    "    else:\n",
    "        return 'Late'\n",
    "\n",
    "data_no_outliers['Part of Month'] = data_no_outliers['TRANS DATE'].dt.day.apply(part_of_month)\n",
    "\n",
    "# Displaying the new features\n",
    "data_no_outliers[['TRANS DATE', 'Day of Week', 'Month', 'Part of Month']].head()\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Distribution of transactions over days of the week\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.countplot(data=data_no_outliers, x='Day of Week', order=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "plt.title('Transactions by Day of Week')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Distribution of transactions over months\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.countplot(data=data_no_outliers, x='Month', order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'])\n",
    "plt.title('Transactions by Month')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Distribution of transactions in parts of the month\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.countplot(data=data_no_outliers, x='Part of Month')\n",
    "plt.title('Transactions by Part of Month')\n",
    "\n",
    "# Distribution of 'ORIGINAL GROSS AMT'\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(data_no_outliers['ORIGINAL GROSS AMT'], bins=30, kde=True)\n",
    "plt.title('Distribution of Original Gross Amount')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of transactions for 'Directorate' and 'MERCHANT NAME'\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Transactions Count by Directorate\n",
    "plt.subplot(2, 1, 1)\n",
    "data_no_outliers['Directorate'].value_counts().head(10).plot(kind='bar', color='skyblue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Top 10 Directorates by Number of Transactions')\n",
    "plt.xlabel('Directorate')\n",
    "plt.ylabel('Number of Transactions')\n",
    "\n",
    "# Transactions Count by MERCHANT NAME\n",
    "plt.subplot(2, 1, 2)\n",
    "data_no_outliers['MERCHANT NAME'].value_counts().head(10).plot(kind='bar', color='skyblue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Top 10 Merchants by Number of Transactions')\n",
    "plt.xlabel('Merchant Name')\n",
    "plt.ylabel('Number of Transactions')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the average ORIGINAL GROSS AMT for each Directorate and Merchant\n",
    "avg_gross_amt_by_directorate = data_no_outliers.groupby('DIRECTORATE')['ORIGINAL GROSS AMT'].mean().sort_values(ascending=False).head(10)\n",
    "avg_gross_amt_by_merchant = data_no_outliers.groupby('MERCHANT NAME')['ORIGINAL GROSS AMT'].mean().sort_values(ascending=False).head(20)\n",
    "\n",
    "# Plotting the average ORIGINAL GROSS AMT for Directorates and Merchants\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Average Gross Amount by Directorate\n",
    "plt.subplot(2, 1, 1)\n",
    "aa = avg_gross_amt_by_directorate.plot(kind='barh', color='skyblue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Top 10 Directorates by Average Transaction Amount')\n",
    "plt.xlabel('Directorate')\n",
    "plt.ylabel('Average Transaction Amount (ORIGINAL GROSS AMT)')\n",
    "aa.invert_yaxis()\n",
    "\n",
    "# Average Gross Amount by Merchant\n",
    "plt.subplot(2, 1, 2)\n",
    "bb = avg_gross_amt_by_merchant.plot(kind='barh', color='skyblue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Top 20 Merchants by Average Transaction Amount')\n",
    "plt.xlabel('Merchant Name')\n",
    "plt.ylabel('Average Transaction Amount (ORIGINAL GROSS AMT)')\n",
    "bb.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_directorate = data_no_outliers['DIRECTORATE'].value_counts().idxmax()\n",
    "\n",
    "# Filtering data for the top Directorate category\n",
    "top_directorate_data = data_no_outliers[data_no_outliers['DIRECTORATE'] == top_directorate]\n",
    "\n",
    "# Finding the top 10 merchants in this Directorate category\n",
    "top_merchants_in_top_directorate = top_directorate_data['MERCHANT NAME'].value_counts().head(10)\n",
    "\n",
    "# Plotting the top 10 merchants in the top Directorate category\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = top_merchants_in_top_directorate.plot(kind='barh', color='skyblue')\n",
    "plt.title(f'Top 10 Merchants in {top_directorate} Directorate')\n",
    "plt.xlabel('Number of Transactions')\n",
    "plt.ylabel('Merchant Name')\n",
    "ax.invert_yaxis()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_proj1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
